{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ded7940-970b-4f6d-af4b-f205d026e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'requests' module does not fit in this case 'cause of anti-bot cloudflare protection\n",
    "# instead, use cloudscraper (https://pypi.org/project/cloudscraper/)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time, pyautogui, cloudscraper, json\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48969b-415b-4462-a9e0-feea02bc0b03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca63acde-21fb-4735-bcd6-64de552569aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_the_content(page_source, result):\n",
    "    prev = len(result)\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    res = soup.find_all(name = 'a', attrs = {'class': 'olx-ad-card__link-wrapper', 'data-ds-component': 'DS-NewAdCard-Link'})\n",
    "    out = [link['href'] for link in res if link['href'] not in result]\n",
    "    result.extend(out)\n",
    "    return prev, len(result)\n",
    "\n",
    "def scrape_page(url):\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    page = scraper.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    r = {}\n",
    "    r['url'] = url\n",
    "    \n",
    "    try:\n",
    "        r['titulo_anuncio'] = soup.find(name = 'h1', attrs = {'class': 'ad__sc-45jt43-0 htAiPK sc-VigVT ZEEcn', 'data-ds-component': 'DS-Text'}).text\n",
    "    except:\n",
    "        r['titulo_anuncio'] = ''\n",
    "        \n",
    "    try:\n",
    "        aux = soup.find(name = 'span', attrs = {'class': 'ad__sc-1oq8jzc-0 dWayMW sc-VigVT dHNWJq', 'data-ds-component': 'DS-Text'}).text\n",
    "        aux2 = aux.split(' ')\n",
    "        r['data'] = aux2[2]\n",
    "        r['hora'] = aux2[-1]\n",
    "    except:\n",
    "        r['data'] = ''\n",
    "        r['hora'] = ''\n",
    "    \n",
    "    try:\n",
    "        r['codigo'] = soup.find(name = 'span', attrs = {'class': 'ad__sc-16iz3i7-0 hjLLUR sc-VigVT dHNWJq', 'data-ds-component': 'DS-Text'}).text.split(' ')[-1]\n",
    "    except:\n",
    "        r['codigo'] = ''\n",
    "        \n",
    "    try:\n",
    "        r['preco'] = soup.find(name = 'h2', attrs = {'class': 'ad__sc-n916xm-3 caQRQy sc-VigVT honhBH', 'data-ds-component': 'DS-Text'}).text.split(' ')[-1]\n",
    "    except:\n",
    "        r['preco'] = ''\n",
    "        \n",
    "    try:\n",
    "        aux = soup.find_all(name = 'span', attrs = {'class': 'ad__sc-2iplj6-3 xIkHt sc-VigVT rycv', 'data-ds-component': 'DS-Text'})    \n",
    "        r['valor_condominio'] = aux[0].text\n",
    "        r['valor_iptu'] = aux[1].text\n",
    "    except:\n",
    "        r['valor_condominio'] = ''\n",
    "        r['valor_iptu'] = ''\n",
    "        \n",
    "    try:\n",
    "        r['descricao'] = soup.find(name = 'span', attrs = {'class': 'ad__sc-1sj3nln-1 fMgwdS sc-VigVT jcyavR', 'data-ds-component': 'DS-Text'}).text\n",
    "    except:\n",
    "        r['descricao'] = ''\n",
    "    \n",
    "    aux = soup.find(name = 'div', attrs = {'class': 'ad__sc-10rz2dk-2 fxAfax'})\n",
    "    try:           \n",
    "        aux2 = [i.text for i in aux.find_all('p')]\n",
    "    except:\n",
    "        r['endereco'] = ''\n",
    "        r['bairro'] = ''\n",
    "        r['cidade'] = ''\n",
    "        r['estado'] = ''\n",
    "        r['cep'] = ''\n",
    "    else:\n",
    "        r['endereco'] = aux2[0]\n",
    "        \n",
    "        aux3 = aux2[1].split('-')\n",
    "        if len(aux3) > 2:\n",
    "            r['bairro'] = '-'.join(aux3[:-1]).strip()\n",
    "        else:\n",
    "            r['bairro'] = aux3[0]\n",
    "\n",
    "        aux4 = aux3[-1].split(',')    \n",
    "        r['cidade'] = aux4[0].strip()    \n",
    "        r['estado'] = aux4[1].strip()    \n",
    "        r['cep'] = aux2[2].split(' ')[-1]    \n",
    "    \n",
    "    aux = soup.find_all(name = 'div', attrs = {'class': 'ad__sc-1yh67o8-2 dUSUNf'})\n",
    "    try:        \n",
    "        r['quartos'] = aux[0].find('a').text\n",
    "    except:\n",
    "        r['quartos'] = ''\n",
    "    \n",
    "    try:\n",
    "        r['areas'] = aux[1].find('p').text\n",
    "    except:\n",
    "        r['areas'] = ''\n",
    "    \n",
    "    try:\n",
    "        r['vagas_carros'] = aux[2].find('p').text\n",
    "    except:\n",
    "        r['vagas_carros'] = ''\n",
    "    \n",
    "    try:\n",
    "        r['banheiros'] = aux[3].find('p').text\n",
    "    except:\n",
    "        r['banheiros'] = ''\n",
    "        \n",
    "    aux = soup.find_all(name = 'div', attrs = {'class': 'ad__sc-1x4pdhw-1 deKSko'})\n",
    "    try:\n",
    "        r['car_imovel'] = [i.text for i in aux[0]]        \n",
    "    except:\n",
    "        r['car_imovel'] = []\n",
    "    \n",
    "    try:\n",
    "        r['car_condominio'] = [i.text for i in aux[1]]\n",
    "    except:\n",
    "        r['car_condominio'] = []\n",
    "    \n",
    "    aux = soup.find(name = 'script', attrs = {'id': 'initial-data'})\n",
    "    aux = json.loads(aux['data-json'])\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        cep = aux['ad']['location']['zipcode']\n",
    "        signature = aux['ad']['signedPostalCode']\n",
    "        \n",
    "        search_pos = f'https://busca-cep.olx.com.br/geocode?cep={cep}&signature={signature}'\n",
    "        page = scraper.get(search_pos)\n",
    "        aux2 = json.loads(page.text)\n",
    "        r['lat'] = aux2['lat']\n",
    "        r['lon'] = aux2['lng']\n",
    "    except:\n",
    "        r['lat'] = ''\n",
    "        r['lon'] = ''\n",
    "        \n",
    "    r['json'] = aux\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83151bf1-e748-464f-a103-1491c46ebd9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Get links with 'Speedy Gonzales' robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab983c2-7ce7-4274-b1aa-8f33552cddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are not supposed to change these constant values\n",
    "olx_url = 'https://www.olx.com.br/imoveis/venda/estado-pa/regiao-de-belem/belem'\n",
    "width = height = 800\n",
    "scroll = 1500\n",
    "driver = webdriver.Firefox()\n",
    "links = []\n",
    "\n",
    "# Position of the screen (not required)\n",
    "ss_w, ss_h = pyautogui.size()\n",
    "driver.set_window_size(width, height)\n",
    "driver.set_window_position(ss_w / 2 - width / 2, ss_h / 2 - height / 2)\n",
    "\n",
    "# Change according to what you want to grab\n",
    "# You will have around links_per_page * len(pages) urls\n",
    "links_per_page = 10 # ads per page\n",
    "pages = list(range(1, 6)) # pages to iterate\n",
    "\n",
    "\n",
    "f = IntProgress(min = 0, max = len(pages), description = 'Progress:')\n",
    "display(f) # display the bar\n",
    "\n",
    "for page_number in pages:\n",
    "    pixels_h = scroll\n",
    "    counter = 0\n",
    "    driver.get(olx_url + f'?o={page_number}')\n",
    "    # print(f'Page {page_number}:')\n",
    "    # print(f'---Total links: ', end = ' ')\n",
    "    \n",
    "    while len(links) < links_per_page * page_number and counter < 2:\n",
    "        driver.execute_script(f'window.scrollTo(0, {pixels_h});')\n",
    "        time.sleep(1)\n",
    "        prev, act = get_part_of_the_content(driver.page_source, links)\n",
    "        pixels_h += scroll        \n",
    "        # print(f'{len(links)}', end = ' ')\n",
    "        if prev == act:\n",
    "            counter += 1\n",
    "    f.value += 1\n",
    "    # print('')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db05600-caaa-44a0-9896-62bbb8037d91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scrape data from each ad page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a81c3-60ed-4b97-8cdf-fcaf0ba3f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = []\n",
    "\n",
    "f = IntProgress(min = 0, max = len(links), description = 'Progress:')\n",
    "display(f) # display the bar\n",
    "\n",
    "for l in links:\n",
    "    ads.append(scrape_page(l))\n",
    "    f.value += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
